{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto Final: Radio Machine Learning Learning 2016.\n",
        "\n",
        "**Autores:**  \n",
        "* Jackeline Morales Hernandez\n",
        "\n",
        "* Jose David Ortiz Miranda\n",
        "\n",
        "**Tratamiento de Señales III**\n",
        "\n",
        "**Facultad de Ingeniería**\n",
        "\n",
        "**Universidad de Antioquia**"
      ],
      "metadata": {
        "id": "08N7PyybEdg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown '1Yuj_rq3EEITslOQ0AHEl2HKzDovMQH3K'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wDoWHkMEwk1",
        "outputId": "78a039c3-e449-439b-cbd0-52e5f9ab08b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yuj_rq3EEITslOQ0AHEl2HKzDovMQH3K\n",
            "To: /content/RML2016.10a_dict.pkl\n",
            "100% 641M/641M [00:10<00:00, 60.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle, sys\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow.keras.models as models\n",
        "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import *\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "#from tensorflow.keras.optimizers import adam"
      ],
      "metadata": {
        "id": "uNhezWZhFSpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset ...\n",
        "#  You will need to seperately download (radioml.com) or generate this file\n",
        "with open(\"RML2016.10a_dict.pkl\", 'rb') as f:\n",
        "    Xd = pickle.load(f, encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "Hv-ekmhXGPZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Acceder con los keys a los dato se Xd que esta en formato Pickle***"
      ],
      "metadata": {
        "id": "ExyrBFKALXd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate modulations and signal-to-noise ratios contained in dataset\n",
        "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])"
      ],
      "metadata": {
        "id": "eEkfBZYuH8Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('These are the modulation schemes in the dataset: {}'.format(mods))\n",
        "# print('These are the signal to noise ratios in the dataset: {}'.format(snrs))"
      ],
      "metadata": {
        "id": "fyu1Sn-VIAAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Función para reconstruccion de la señal, aplicando:***\n",
        "\n",
        "$$\\text{Modulated Carrier RF} = I \\cos(2\\pi ft) + Q \\sin(2 \\pi ft)$$"
      ],
      "metadata": {
        "id": "llPutmxGLOTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Compute the original RF signal from the I & Q components.\n",
        "'''\n",
        "def reconstructSignal(s):\n",
        "    I = s[0,:]   # I component\n",
        "    Q = s[1,:]   # Q component\n",
        "    n = len(I)   # number of samples\n",
        "    f = 1e6     # Carrier frequency\n",
        "    return I*np.cos(2*np.pi*f*np.arange(n)) + Q*np.sin(2*np.pi*f*np.arange(n))"
      ],
      "metadata": {
        "id": "slhKztPQLNOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Se recorren todos los datos con todas la modulaciones y sus respectivas relaciones SNR***"
      ],
      "metadata": {
        "id": "AJ1_4sriMfjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
        "X = []\n",
        "lbl = []\n",
        "for mod in mods:\n",
        "    for snr in snrs:\n",
        "        X.append(Xd[(mod,snr)])\n",
        "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
        "X = np.vstack(X)"
      ],
      "metadata": {
        "id": "l0-7vOgfMyD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Se añade Ruido Blanco Gaussiano***"
      ],
      "metadata": {
        "id": "dw4hm08eNHBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numNoiseSamples = 20000 # Add 20,000 samples of just Gaussian noise\n",
        "for i in range(numNoiseSamples):\n",
        "    lbl.append(['Noise', snrs[i%len(snrs)]])\n",
        "\n",
        "mods.append('Noise')\n",
        "X_noise = np.random.normal(0, 0.01, [numNoiseSamples, 2, 128])\n",
        "X = np.concatenate((X, X_noise))  # Add noise to the end of the data"
      ],
      "metadata": {
        "id": "D1599sUBNSDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Se distingue cada modulación como con un código binario***"
      ],
      "metadata": {
        "id": "pN3BWvlCOZZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbl = np.array(lbl) # Convert from list to numpy array (for speed)\n",
        "\n",
        "# This will encode the labels into a binary array.\n",
        "# This is referred to as One-Hot.\n",
        "# Each element of the array is either 1 or 0.\n",
        "# The position of the element refers to which type of modulation.\n",
        "# So if there are 11 modulation types, then the\n",
        "# array is length 11.\n",
        "# e.g.  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] codes '8PSK'\n",
        "#       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] codes 'AM-DSB'\n",
        "#       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] codes 'WBFM'\n",
        "# We'll use this to compare the output of the neural network\n",
        "# and help train the model to give the lowest error (in this case\n",
        "# the cost will be the cross-entropy).\n",
        "y_encoded = LabelBinarizer().fit_transform(lbl[:,0])\n",
        "\n",
        "# print(y_encoded.shape)\n"
      ],
      "metadata": {
        "id": "kinIU8zLOjIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bacfc22-6eb1-4420-8e55-029aeb44aade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Dividimos entre datos de entrenamiento y test***"
      ],
      "metadata": {
        "id": "yY1i431qQdQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition the data\n",
        "#  into training and test sets of the form we can train/test on\n",
        "#  while keeping SNR and Mod labels handy for each\n",
        "np.random.seed(2016)\n",
        "n_examples = X.shape[0]\n",
        "n_train = int(n_examples * 0.7)\n",
        "\n",
        "# Now get a list of random indicies to sample for the training set\n",
        "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)\n",
        "\n",
        "# Testing set is whatever indicies are left over\n",
        "test_idx = list(set(range(0,n_examples))-set(train_idx))\n",
        "\n",
        "# Now split up the data by the random indicies\n",
        "X_train = X[train_idx]\n",
        "X_test =  X[test_idx]\n",
        "Y_train = y_encoded[train_idx]\n",
        "Y_test = y_encoded[test_idx]"
      ],
      "metadata": {
        "id": "1-OcaYhwQjJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Para ver los datos anteriormente sacados***"
      ],
      "metadata": {
        "id": "4wqRJvbyQvDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Training set is {}'.format(np.shape(X_train)))\n",
        "# print('Test set is {}'.format(np.shape(X_test)))"
      ],
      "metadata": {
        "id": "paB8YzhGQ1eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in_shp = list(X_train.shape[1:])   # This is the input shape of 2 channels x 128 time samples\n",
        "# print(X_train.shape, in_shp)\n",
        "# classes = mods\n",
        "#  print(classes)"
      ],
      "metadata": {
        "id": "ZmZEjATkQ4kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Tarea: Codificar al menos 2 arquitecturas neuronales para evaluar el reconocimiento de la modulación***"
      ],
      "metadata": {
        "id": "qMXZQgr8RGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN Neural Net model using tensorflow  --\n",
        "#  - Reshape [N,2,128] to [N,2,128,1] on input\n",
        "#  - Pass through 3 2DConv/ReLu layers\n",
        "#  - Pass through 2 Dense layers (ReLu and Softmax)\n",
        "#  - Perform categorical cross entropy optimization\n",
        "\n",
        "dr = 0.6 # dropout rate (%) = percentage of neurons to randomly lose each iteration\n",
        "model = models.Sequential()  # Neural network is a set of sequential layers\n",
        "#model.add(Convolution2D(...))\n",
        "'''\n",
        "Your code goes here!\n",
        "....\n",
        "'''\n",
        "# model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rdjAZEAeRag5",
        "outputId": "dbca83b2-182e-4299-9436-3b4ef4323eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYour code goes here!\\n....\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend\n",
        "img_rows, img_cols = 2, 128\n",
        "'''\n",
        "if backend.image_dim_ordering() == 'th':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "'''\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "print('X_train:', X_train.shape)\n",
        "print('X_test:', X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHPUIrw5eMLW",
        "outputId": "fae34fc8-56dd-46f1-a7f2-c8fba89c80ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (168000, 2, 128, 1)\n",
            "X_test: (72000, 2, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of convolutional filters to use\n",
        "nb_filters = 10\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "\n",
        "model.add(Conv2D(nb_filters, kernel_size,\n",
        "                 padding='valid',\n",
        "                 input_shape=input_shape,\n",
        "                 activation='relu'))\n",
        "model.add(Conv2D(nb_filters, kernel_size,\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "vof_RsK-flGP",
        "outputId": "8ccfc849-82d6-400f-a27f-8a399fceb7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e37a262546ed>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model.add(Conv2D(nb_filters, kernel_size,\n\u001b[0m\u001b[1;32m     10\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_4\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_4/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_4/Conv2D/ReadVariableOp)' with input shapes: [?,2,128,1], [3,3,1,10].\n\nCall arguments received by layer \"conv2d_4\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 128, 1), dtype=float32)"
          ]
        }
      ]
    }
  ]
}